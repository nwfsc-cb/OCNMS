---
title: "Summarise Relief Data OCNMS July 2020"
author: Genoa Sullaway
output: html_document
---


\n This uses 2016-2019 data, 2015 Relief Data uses quadrats and is inconsistent with later methods, calculates mean and standard deviation (specific to beta distributed data) for a few different flavors of data summaries, summarized below. 

\n Data frames created here are saved in: OCNMS/Data/Summarized data

\n 1. dat.transect.summary = RELIEF_Transect_Level_Summary.csv: This has percent cover across TRANSECT/ZONE/SITE/YEAR. Transect is lowest level of replication here. There are only 11 transects in whole dataset that have less than 30 upc counts (aka an incomplete transect)... In the cases where it is less than 30 I get total % cover from the total points sampled for that transect. 
\n 2. dat.zone.summary = RELIEF_Zone_Level_Summary.csv: This has mean percent cover and variance across ZONE/SITE/YEAR. 
\n 3. dat.site.summary = RELIEF_Site_Level_Summary.csv: This has mean percent cover and variance across SITES. 

```{r setup,message=FALSE, warning=FALSE}
#to load data make sure directory is set to knit from project directory
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
 
library(here)
library(tidyverse)
#library(knitr)
library(viridis)
library(vegan)
 
substrate_codes <- read_csv(here("Data","CSV_2015_on","substrate_codes.csv"))
 
#2016 --> 2019
dat.relief <- read_csv(here("Data","CSV_2015_on","NWFSC_UPC_ALLYEARS_data_2019.csv"), col_types = cols(SIDE = col_character())) %>% 
              filter(CATEGORY == "RELIEF") %>%
              select(1:20) %>%
              rename(DEPTH.FT= "DEPTH (FT)")  %>%
              #mutate(QUADRAT = NA, PERCENT.COVER = NA) %>% #will circle back tothis and get % cover for 2016 to 2019 data 
              group_by(SITE, YEAR,SIDE,ZONE,TRANSECT, CLASSCODE) %>% #combine across segments 
              summarise(COUNT = sum(COUNT)) %>%
              spread(CLASSCODE, COUNT) %>%
              ungroup() %>%
              replace(is.na(.), 0) %>%
              dplyr::mutate(Sum = rowSums(.[,6:9])) %>%
              tidyr::gather(6:9, key = "CLASSCODE", value = "COUNT") %>%
              mutate(percent_cover = COUNT/Sum, CLASSCODE = factor(CLASSCODE, levels=c("0-10cm","10cm-1m","1m-2m",">2m")))

dat.zone.summary <- dat.relief %>%
  group_by(YEAR,SITE,ZONE,CLASSCODE) %>% 
   summarise(#mean_percent_cover = mean(percent_cover), variance = var(percent_cover)) %>%  
    MEAN=mean(percent_cover), 
    SD=sqrt(MEAN * (1 - MEAN)), 
    N=length(percent_cover), 
    SE=SD/sqrt(N), .groups = "keep" ) %>%
  ungroup() %>%
  mutate(SITE = factor(SITE, levels = c("Destruction Island","Cape Johnson", "Cape Alava","Tatoosh Island","Neah Bay")))
 
dat.site.summary <- dat.relief %>%
  group_by(SITE,CLASSCODE) %>% 
  summarise(#mean_percent_cover = mean(percent_cover), variance = var(percent_cover)) %>%  
    MEAN=mean(percent_cover), 
    SD=sqrt(MEAN * (1 - MEAN)), 
    N=length(percent_cover), 
    SE=SD/sqrt(N), .groups = "keep" ) %>%
  ungroup() %>%
  mutate(SITE = factor(SITE, levels = c("Destruction Island","Cape Johnson", "Cape Alava","Tatoosh Island","Neah Bay")))

  write.csv(dat.relief, here("Data","Summarized data","RELIEF_Transect_Level_Summary.csv"))
  write.csv(dat.zone.summary, here("Data","Summarized data","RELIEF_Zone_Level_Summary.csv"))
  write.csv(dat.site.summary, here("Data","Summarized data","RELIEF_Site_Level_Summary.csv"))
  

```

```{r}
ggplot(dat.zone.summary,aes(x=YEAR)) +
  geom_bar(aes(y=MEAN, fill= CLASSCODE), position="fill", stat="identity",width = 0.95
           )+
  scale_fill_viridis(discrete = T) + #,  guide = guide_legend(reverse = TRUE), name = "Stock") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "left",aspect.ratio = 3/1)  +
  theme_classic()+
  ylab("Mean Percent Cover") +
  xlab("Year") +
  facet_grid(ZONE~SITE) +
 # coord_flip() +
  scale_x_continuous(expand = c(0,0))+
  scale_y_continuous(expand = c(0,0)) +
  theme(legend.position="left") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1), strip.text.x = element_text(size = 7))

```


```{r}
ggplot(dat.site.summary,aes(x=SITE)) +
  geom_bar(aes(y=MEAN, fill= CLASSCODE), position="fill", stat="identity",width = 0.95
           )+
  scale_fill_viridis(discrete = T) + #,  guide = guide_legend(reverse = TRUE), name = "Stock") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "left",aspect.ratio = 3/1)  +
  theme_classic()+
  ylab("Mean Percent Cover") +
  xlab("Year") +
  #facet_wrap(~ZONE, nrow =1) +
  scale_x_discrete(expand = c(0,0))+
  scale_y_continuous(expand = c(0,0)) +
  theme(legend.position="left") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1), strip.text.x = element_text(size = 7))

```


```{r calculate Simpsons Diversity}

#use dat.relief to get Simpsons index for every transect 

#make a unique ID and make a list of DFs for those then loop through those DFs 

dat.id<- dat.relief %>% 
  unite("ID", c("SITE","YEAR","SIDE","ZONE","TRANSECT"))
```











 